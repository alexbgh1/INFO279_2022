{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto (obtención dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- General ---\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Data ---\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "import numpy as np\n",
    "\n",
    "# --- Conexión ---\n",
    "import elasticsearch\n",
    "\n",
    "# --- Procesamiento lenguaje: spacy ---\n",
    "import spacy\n",
    "# from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Configuración ---\n",
    "# User password\n",
    "password = os.environ.get('SOPHIA2')\n",
    "# --- Funciones ---\n",
    "# Cargar paquete de español mediano\n",
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización peticiones Sophia2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Nombre archivo ---\n",
    "# nombre_archivo = 'data'\n",
    "\n",
    "# # --- Parameters queries ---\n",
    "# # País\n",
    "# country=\"chile\"\n",
    "# # Fecha\n",
    "# from_=\"2010-01-01\"\n",
    "# to_=\"2010-12-31\"\n",
    "# media_outlets = ['elciudadano']\n",
    "# nombre_archivo = nombre_archivo + '_' + from_ + '_' + to_ + '.csv'\n",
    "\n",
    "\n",
    "# # --- Data Connection ---\n",
    "# IP = \"search.sophia2.org\"\n",
    "# PORT = 9200\n",
    "# USER= \"elastic\"\n",
    "# PASS= password\n",
    "\n",
    "# # --- Connection ---\n",
    "# es = elasticsearch.Elasticsearch(\n",
    "#     IP,\n",
    "#     http_auth=(USER, PASS)\n",
    "# )\n",
    "\n",
    "# # --- Queries ---\n",
    "# query = { \n",
    "#     \"bool\": { \n",
    "#     \"filter\": [\n",
    "#         {\"range\": {\n",
    "#       \"date\": {\n",
    "#         \"gte\": from_,\n",
    "#         \"lt\": to_\n",
    "#       }\n",
    "#       }},\n",
    "\n",
    "#         { \"term\":  { \"country\": country }},\n",
    "#         { \"terms\":  { \"media_outlet\": media_outlets }} \n",
    "#     ]\n",
    "#     }  \n",
    "# }\n",
    "# # --- Confirmación de conexión ---\n",
    "# res = es.search(index=\"news\", query=query, size=10000)\n",
    "# print(\"Son %d noticias encontradas...\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automatico(fechas):\n",
    "    # --- Data Connection ---\n",
    "    IP = \"search.sophia2.org\"\n",
    "    PORT = 9200\n",
    "    USER= \"elastic\"\n",
    "    PASS= 'dummy_elastic'\n",
    "\n",
    "    # --- Connection ---\n",
    "    es = elasticsearch.Elasticsearch(\n",
    "        IP,\n",
    "        http_auth=(USER, PASS)\n",
    "    )\n",
    "    \n",
    "    for from_,to_ in (fechas):\n",
    "        # --- Nombre archivo ---\n",
    "        nombre_archivo = 'data'\n",
    "        # --- Parameters queries ---\n",
    "        # País\n",
    "        country=\"chile\"\n",
    "        # Fecha\n",
    "        media_outlets = ['elciudadano']\n",
    "        nombre_archivo = nombre_archivo + '_' + from_ + '_' + to_ + '.csv'\n",
    "\n",
    "        query = { \n",
    "            \"bool\": { \n",
    "            \"filter\": [\n",
    "                {\"range\": {\n",
    "            \"date\": {\n",
    "                \"gte\": from_,\n",
    "                \"lt\": to_\n",
    "            }\n",
    "            }},\n",
    "\n",
    "                { \"term\":  { \"country\": country }},\n",
    "                { \"terms\":  { \"media_outlet\": media_outlets }} \n",
    "            ]\n",
    "            }  \n",
    "        }\n",
    "        res = es.search(index=\"news\", query=query, size=10000)\n",
    "        if (len(res) == 10000):\n",
    "            return f'Revisar: {from_},{to_}'\n",
    "\n",
    "        # --- Columnas Dataframe ---\n",
    "        data = {'id_news':[],'country':[],'media_outlet':[],'url':[],'title':[],'text':[],'date':[]}\n",
    "        df = pd.DataFrame(data)  \n",
    "\n",
    "        # --- Información Dataframe---\n",
    "        for hit in res['hits']['hits']:\n",
    "\n",
    "            id_news = hit['_source']['id_news']\n",
    "            country = hit['_source']['country']\n",
    "            media_outlet = hit['_source']['media_outlet']\n",
    "            url = hit['_source']['url']\n",
    "            title = hit['_source']['title']\n",
    "            text = hit['_source']['text']\n",
    "            date = hit['_source']['date']\n",
    "            \n",
    "            new_row = {'id_news':int(id_news), 'country':country, 'media_outlet':media_outlet, 'url':url, 'title':title, 'text':text, 'date':date}\n",
    "            df = pd.concat([df, pd.DataFrame.from_records([new_row])])\n",
    "            # df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "\n",
    "        # Reseteamos el índice del dataframe para que sea secuencial, sino serían puros 0.\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        # --- Elimina resiuduos ---\n",
    "        # Elimina duplicados\n",
    "        df = df.drop_duplicates(subset='url', keep='first')\n",
    "        # Elimina textos vacíos o con muy poco contenido\n",
    "        df = df[df['text'].str.len() > 50]\n",
    "\n",
    "        # Dejamos id_news,media_outlet,url,title,date\n",
    "        df = df[['id_news','url','title','date']]\n",
    "\n",
    "        # --- Info Archivo ---\n",
    "        print(\"El archivo se llama:\")\n",
    "        print(nombre_archivo)\n",
    "\n",
    "        # --- Guarda archivo ---\n",
    "        df.to_csv(\"./data/\"+nombre_archivo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo se llama:\n",
      "data_2010-01-01_2010-06-30.csv\n",
      "El archivo se llama:\n",
      "data_2010-06-30_2010-12-31.csv\n",
      "El archivo se llama:\n",
      "data_2011-01-01_2011-06-30.csv\n",
      "El archivo se llama:\n",
      "data_2011-06-30_2011-12-31.csv\n",
      "El archivo se llama:\n",
      "data_2012-01-01_2012-06-30.csv\n",
      "El archivo se llama:\n",
      "data_2012-06-30_2012-12-31.csv\n",
      "El archivo se llama:\n",
      "data_2013-01-01_2013-06-30.csv\n",
      "El archivo se llama:\n",
      "data_2013-06-30_2013-12-31.csv\n",
      "El archivo se llama:\n",
      "data_2014-01-01_2014-06-30.csv\n",
      "El archivo se llama:\n",
      "data_2014-06-30_2014-12-31.csv\n",
      "El archivo se llama:\n",
      "data_2015-01-01_2015-06-30.csv\n",
      "El archivo se llama:\n",
      "data_2015-06-30_2015-12-31.csv\n",
      "El archivo se llama:\n",
      "data_2016-01-01_2016-06-30.csv\n",
      "El archivo se llama:\n",
      "data_2016-06-30_2016-12-31.csv\n",
      "El archivo se llama:\n",
      "data_2017-01-01_2017-06-30.csv\n",
      "El archivo se llama:\n",
      "data_2017-06-30_2017-12-31.csv\n",
      "El archivo se llama:\n",
      "data_2018-01-01_2018-06-30.csv\n",
      "El archivo se llama:\n",
      "data_2018-06-30_2018-12-31.csv\n",
      "El archivo se llama:\n",
      "data_2019-01-01_2019-06-30.csv\n",
      "El archivo se llama:\n",
      "data_2019-06-30_2019-12-31.csv\n",
      "El archivo se llama:\n",
      "data_2020-01-01_2020-06-30.csv\n",
      "El archivo se llama:\n",
      "data_2020-06-30_2020-12-31.csv\n",
      "El archivo se llama:\n",
      "data_2021-01-01_2021-06-30.csv\n",
      "El archivo se llama:\n",
      "data_2021-06-30_2021-12-31.csv\n"
     ]
    }
   ],
   "source": [
    "from_=\"2010-01-01\"\n",
    "to_=\"2010-06-31\"\n",
    "fechas = [\n",
    "    ('2010-01-01','2010-06-30'),\n",
    "    ('2010-06-30','2010-12-31'),\n",
    "    ('2011-01-01','2011-06-30'),\n",
    "    ('2011-06-30','2011-12-31'),\n",
    "    ('2012-01-01','2012-06-30'),\n",
    "    ('2012-06-30','2012-12-31'),\n",
    "    ('2013-01-01','2013-06-30'),\n",
    "    ('2013-06-30','2013-12-31'),\n",
    "    ('2014-01-01','2014-06-30'),\n",
    "    ('2014-06-30','2014-12-31'),\n",
    "    ('2015-01-01','2015-06-30'),\n",
    "    ('2015-06-30','2015-12-31'),\n",
    "    ('2016-01-01','2016-06-30'),\n",
    "    ('2016-06-30','2016-12-31'),\n",
    "    ('2017-01-01','2017-06-30'),\n",
    "    ('2017-06-30','2017-12-31'),\n",
    "    ('2018-01-01','2018-06-30'),\n",
    "    ('2018-06-30','2018-12-31'),\n",
    "    ('2019-01-01','2019-06-30'),\n",
    "    ('2019-06-30','2019-12-31'),\n",
    "    ('2020-01-01','2020-06-30'),\n",
    "    ('2020-06-30','2020-12-31'),\n",
    "    ('2021-01-01','2021-06-30'),\n",
    "    ('2021-06-30','2021-12-31')\n",
    "]\n",
    "automatico(fechas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Columnas Dataframe ---\n",
    "# data = {'id_news':[],'country':[],'media_outlet':[],'url':[],'title':[],'text':[],'date':[]}\n",
    "# df = pd.DataFrame(data)  \n",
    "\n",
    "# # --- Información Dataframe---\n",
    "# for hit in res['hits']['hits']:\n",
    "\n",
    "#     id_news = hit['_source']['id_news']\n",
    "#     country = hit['_source']['country']\n",
    "#     media_outlet = hit['_source']['media_outlet']\n",
    "#     url = hit['_source']['url']\n",
    "#     title = hit['_source']['title']\n",
    "#     text = hit['_source']['text']\n",
    "#     date = hit['_source']['date']\n",
    "    \n",
    "#     new_row = {'id_news':int(id_news), 'country':country, 'media_outlet':media_outlet, 'url':url, 'title':title, 'text':text, 'date':date}\n",
    "#     df = pd.concat([df, pd.DataFrame.from_records([new_row])])\n",
    "#     # df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "\n",
    "# # Reseteamos el índice del dataframe para que sea secuencial, sino serían puros 0.\n",
    "# df = df.reset_index(drop=True)\n",
    "\n",
    "# # --- Elimina resiuduos ---\n",
    "# # Elimina duplicados\n",
    "# df = df.drop_duplicates(subset='url', keep='first')\n",
    "# # Elimina textos vacíos o con muy poco contenido\n",
    "# df = df[df['text'].str.len() > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Son {len(df)} noticias encontradas que se utilizarán\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardamos la información CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Info Archivo ---\n",
    "# print(\"El archivo se llama:\")\n",
    "# print(nombre_archivo)\n",
    "\n",
    "# # --- Guarda archivo ---\n",
    "# df.to_csv(\"./data/\"+nombre_archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Con esto tenemos toda la información necesaria para cargar los datos y utilizarlos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fff480b818351191bd10b139918affcc268b35a4173b95bc67c9e000e77644a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
