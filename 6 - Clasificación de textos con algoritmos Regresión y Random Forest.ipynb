{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c436bd",
   "metadata": {},
   "source": [
    "# Clasificación de textos utilizando algoritmos de Regresión Logística y Random Forest (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f343165",
   "metadata": {},
   "source": [
    "- El presente tutorial se inspira de: https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "\n",
    "En este tutorial intentaremos clasificar reseñas de productos Amazon (Alexa) en dos categorías: positivos o negativos (\"Análisis de sentimientos\").\n",
    "\n",
    "Utilizaremos un enfoque simple:\n",
    "- representaremos los textos con representaciones vectoriales \"Bag of Words\"\n",
    "- utilizaremos algoritmos de Machine Learning para aprender modelos a partir de las representaciones vectoriales\n",
    "- evaluaremos los modelos utilizando una matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1119c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-25 18:07:56.524374: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-25 18:08:00.159043: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-25 18:08:00.159279: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-25 18:08:00.525518: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-25 18:08:11.627659: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-25 18:08:11.628034: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-25 18:08:11.628082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Requirement already satisfied: en-core-web-sm==3.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl#egg=en_core_web_sm==3.2.0 in /home/mvernier/.local/lib/python3.8/site-packages (3.2.0)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/mvernier/.local/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (45.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.63.1)\n",
      "Requirement already satisfied: jinja2 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.7.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.5.2)\n",
      "Requirement already satisfied: click<8.1.0 in /home/mvernier/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/mvernier/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.25.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mvernier/.local/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/mvernier/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /home/mvernier/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c0a384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 18:10:08.340345: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-25 18:10:08.542618: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-25 18:10:08.542639: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-25 18:10:08.577228: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-25 18:10:09.336488: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-25 18:10:09.336569: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-25 18:10:09.336579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.4\n"
     ]
    }
   ],
   "source": [
    "#NLP\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(spacy.__version__)\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import string\n",
    "\n",
    "#SKLEARN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression # Regresion Logística\n",
    "\n",
    "#PANDAS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce2ce00",
   "metadata": {},
   "source": [
    "# 2. Dataset: Alexa\n",
    "\n",
    "Vamos a usar un conjunto de datos real: un conjunto de reseñas de productos de Amazon Alexa. Este conjunto de datos viene como un archivo separado por tabulaciones (.tsv). Tiene cinco columnas: \n",
    "- __rating__: se refiere a la calificación que cada usuario dio a Alexa (de 0 a 5). \n",
    "- __fecha__: fecha de la reseña\n",
    "- __variación__: describe el modelo de producto Alexa que el usuario comentó.\n",
    "- __verified_reviews__: contiene el texto del comentario.\n",
    "- __feedback__: contiene un label, 0 o 1, que indica el sentimiento general negativo (0) o positivo (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2235408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading TSV file\n",
    "df_amazon = pd.read_csv(\"data/amazon_alexa.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c5f61d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Love my Echo!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Loved it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Walnut Finish</td>\n",
       "      <td>Sometimes while playing a game, you can answer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>I have had a lot of fun with this thing. My 4 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Music</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3145</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Perfect for kids, adults and everyone in betwe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Listening to music, searching locations, check...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3147</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>I do love these things, i have them running my...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>Only complaint I have is that the sound qualit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3149</th>\n",
       "      <td>4</td>\n",
       "      <td>29-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating       date         variation  \\\n",
       "0          5  31-Jul-18  Charcoal Fabric    \n",
       "1          5  31-Jul-18  Charcoal Fabric    \n",
       "2          4  31-Jul-18    Walnut Finish    \n",
       "3          5  31-Jul-18  Charcoal Fabric    \n",
       "4          5  31-Jul-18  Charcoal Fabric    \n",
       "...      ...        ...               ...   \n",
       "3145       5  30-Jul-18        Black  Dot   \n",
       "3146       5  30-Jul-18        Black  Dot   \n",
       "3147       5  30-Jul-18        Black  Dot   \n",
       "3148       5  30-Jul-18        White  Dot   \n",
       "3149       4  29-Jul-18        Black  Dot   \n",
       "\n",
       "                                       verified_reviews  feedback  \n",
       "0                                         Love my Echo!         1  \n",
       "1                                             Loved it!         1  \n",
       "2     Sometimes while playing a game, you can answer...         1  \n",
       "3     I have had a lot of fun with this thing. My 4 ...         1  \n",
       "4                                                 Music         1  \n",
       "...                                                 ...       ...  \n",
       "3145  Perfect for kids, adults and everyone in betwe...         1  \n",
       "3146  Listening to music, searching locations, check...         1  \n",
       "3147  I do love these things, i have them running my...         1  \n",
       "3148  Only complaint I have is that the sound qualit...         1  \n",
       "3149                                               Good         1  \n",
       "\n",
       "[3150 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 5 records\n",
    "df_amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ea77c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataframe\n",
    "df_amazon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560613b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2893\n",
       "0     257\n",
       "Name: feedback, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feedback Value count\n",
    "df_amazon.feedback.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b450388",
   "metadata": {},
   "source": [
    "#### Partición de los datos en conjuntos de entrenamiento y test para entrenar y evaluar un modelo predictivo\n",
    "\n",
    "Usaremos la mitad de nuestro conjunto de datos como nuestro conjunto de entrenamiento, que incluirá las respuestas correctas. Luego probaremos nuestro modelo usando la otra mitad del conjunto de datos sin darle las respuestas, para ver con qué precisión funciona.\n",
    "\n",
    "Convenientemente, scikit-learn nos da una función incorporada para hacer esto: train_test_split(). Sólo necesitamos decirle el conjunto de características que queremos que se divida (X), las etiquetas contra las que queremos que se realice la prueba (ylabels), y el tamaño que queremos usar para el conjunto de pruebas (representado como un porcentaje en forma decimal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7615f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_amazon['verified_reviews'] # the features we want to analyze\n",
    "ylabels = df_amazon['feedback'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c00f7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           Love my Echo!\n",
       "1                                               Loved it!\n",
       "2       Sometimes while playing a game, you can answer...\n",
       "3       I have had a lot of fun with this thing. My 4 ...\n",
       "4                                                   Music\n",
       "                              ...                        \n",
       "3145    Perfect for kids, adults and everyone in betwe...\n",
       "3146    Listening to music, searching locations, check...\n",
       "3147    I do love these things, i have them running my...\n",
       "3148    Only complaint I have is that the sound qualit...\n",
       "3149                                                 Good\n",
       "Name: verified_reviews, Length: 3150, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06550805",
   "metadata": {},
   "source": [
    "# 3. Preprocesamientos y representación vectorial\n",
    "\n",
    "Crearemos una función personalizada <code>spacy_tokenizer()</code> que acepta una frase como entrada y la procesa en tokens, realizando lemmatización, minúsculas y eliminando palabras stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b74ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our list of punctuation marks\n",
    "punctuations = [\".\",\",\",\"!\",\"?\", \"#\",\"&\"]\n",
    "\n",
    "# Create our list of stopwords\n",
    "stop_words=[\"\"]\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [word.lower_ for word in mytokens]\n",
    "        \n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afeae2",
   "metadata": {},
   "source": [
    "#### Vectorización de los textos en BoW o TF-IDF, con scikit-learn\n",
    "\n",
    "Podemos generar una matriz BoW para nuestros datos de texto usando la clase <code>CountVectorizer</code> de scikit-learn. En el código de abajo, le decimos a CountVectorizer que use la función personalizada spacy_tokenizer que construimos como su tokenizer, y que defina el rango de ngramo que queremos.\n",
    "\n",
    "Los N-gramos son combinaciones de palabras adyacentes en un texto dado, donde _n_ es el número de palabras que se incluyen en las fichas. Por ejemplo, en la frase \"¿Quién ganará la Copa del Mundo de fútbol en 2022? Bigramas sería una secuencia de dos palabras contiguas como \"quién ganará\", \"ganará la\", y así sucesivamente. Así que el parámetro ngram_range que usaremos en el código de abajo establece los límites inferior y superior de nuestros ngramas (usaremos unigramas). Entonces asignaremos los ngramas a bow_vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca5f634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7aab7",
   "metadata": {},
   "source": [
    "Podemos también transformar los textos en vectores para tener los pesos TF-IDF de cada palabra en cada documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "080c1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286d177",
   "metadata": {},
   "source": [
    "# 4. Entrenamiento del modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72626c3d",
   "metadata": {},
   "source": [
    "Es el momento de construir nuestro modelo predictivo. Empezaremos importando el módulo LogisticRegression y creando un objeto clasificador LogisticRegression.\n",
    "\n",
    "Luego, crearemos un pipeline de procesamiento con dos componentes: un vectorizador y algoritmo de clasificación basado en la regresión logística. El vectorizador utiliza preprocesamientos (spacy) y vectorización (scikit-learn) para crear una matriz para representar nuestros textos.\n",
    "\n",
    "Una vez que se construya este pipeline, se aprende el modelo predictivo llamando el método fit()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb46765",
   "metadata": {},
   "source": [
    "- Regresión Logística sobre representacion vectorial \"BoW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c07272a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 CountVectorizer(tokenizer=<function spacy_tokenizer at 0x7f75daf819d0>)),\n",
       "                ('regression-ML', LogisticRegression())])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "modelLR = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "model1 = Pipeline([('preprocessing', bow_vector),\n",
    "                 ('regression-ML', modelLR)])\n",
    "\n",
    "# model generation\n",
    "model1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64eb9bf",
   "metadata": {},
   "source": [
    "- Regresión Logística sobre representacion vectorial \"TF-IDF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "924ce407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x7f75daf819d0>)),\n",
       "                ('regression-ML', LogisticRegression())])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "modelLR = LogisticRegression()\n",
    "\n",
    "model2 = Pipeline([('preprocessing', tfidf_vector),\n",
    "                 ('regression-ML', modelLR)])\n",
    "\n",
    "# model generation\n",
    "model2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c1aff0",
   "metadata": {},
   "source": [
    "- Random Forest sobre representacion vectorial \"BoW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f97bfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 CountVectorizer(tokenizer=<function spacy_tokenizer at 0x7f75daf819d0>)),\n",
       "                ('regression-ML', RandomForestClassifier(random_state=0))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelRF = RandomForestClassifier(random_state=0)\n",
    "\n",
    "model3 = Pipeline([('preprocessing', bow_vector),\n",
    "                 ('regression-ML', modelRF)])\n",
    "\n",
    "# model generation\n",
    "model3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96663a86",
   "metadata": {},
   "source": [
    "# 5. Evaluación del modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d17e731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "Logistic Regression Accuracy: 0.9365079365079365\n",
      "Logistic Regression Precision: 0.94067237969677\n",
      "Logistic Regression Recall: 0.9930410577592206\n"
     ]
    }
   ],
   "source": [
    "# Predicting with a test dataset\n",
    "predicted = model1.predict(X_test)\n",
    "print(predicted)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2de96cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  48   90]\n",
      " [  10 1427]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.35      0.49       138\n",
      "           1       0.94      0.99      0.97      1437\n",
      "\n",
      "    accuracy                           0.94      1575\n",
      "   macro avg       0.88      0.67      0.73      1575\n",
      "weighted avg       0.93      0.94      0.92      1575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluación del rendimiento del clasificador\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predicted)\n",
    "print(confusion_matrix)\n",
    "\n",
    "#Print de la matriz de confusión\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1404e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNMostInformative(vectorizer, model, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(model.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "731a38f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 best: \n",
      "(-3.041985892838304, 'not')\n",
      "(-1.4639288256531542, 'back')\n",
      "(-1.438688201121309, 'terrible')\n",
      "(-1.4171451779980204, 'working')\n",
      "(-1.3169642931766559, ' ')\n",
      "(-1.2651698372293538, 'does')\n",
      "(-1.1789509787712433, 'n’t')\n",
      "(-1.0936740127102857, 'after')\n",
      "(-1.0771754470196913, \"n't\")\n",
      "(-1.0495686882016453, 'stopped')\n",
      "(-1.0268504071732543, 'poor')\n",
      "(-1.003691422925835, 'work')\n",
      "(-0.9464029924581475, 'the')\n",
      "(-0.9369719908618762, 'hardly')\n",
      "(-0.9014437384914352, 'this')\n",
      "(-0.8837361065888238, 'will')\n",
      "(-0.878431219031461, 'low')\n",
      "(-0.8777103527353189, 'features')\n",
      "(-0.8762604661504177, 'no')\n",
      "(-0.8585147323207976, '..')\n",
      "Class 2 best: \n",
      "(2.511588006960702, 'love')\n",
      "(1.813440413650868, 'great')\n",
      "(1.2990465578168795, 'easy')\n",
      "(1.1372186873977983, 'like')\n",
      "(0.8668221351635402, 'my')\n",
      "(0.835829190559999, 'works')\n",
      "(0.7995099451002685, 'alexa')\n",
      "(0.7533529210212707, 'new')\n",
      "(0.7430799895332303, 'for')\n",
      "(0.7413385258455675, 'she')\n",
      "(0.7187801210504172, 'nice')\n",
      "(0.6930040896412816, 'with')\n",
      "(0.6633406673077986, 'learning')\n",
      "(0.640432985063114, 'set')\n",
      "(0.6352786467322894, 'perfect')\n",
      "(0.6306527768766498, 'fun')\n",
      "(0.6111462369981845, 'use')\n",
      "(0.6098557400843176, 'i')\n",
      "(0.6021561372203444, 'good')\n",
      "(0.5740054181220033, 'cool')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvernier/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "printNMostInformative(bow_vector, modelLR, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddabbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f132900e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
